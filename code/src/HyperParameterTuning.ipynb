{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aab9b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cf3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the class\n",
    "class ModelValidator:\n",
    "    # Creating attributes that define and use later\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_top_lr = None\n",
    "        self.cv_top_dt = None\n",
    "        self.top_hp_lr = None\n",
    "        self.top_3_lr = None\n",
    "        self.top_hp_dt = None\n",
    "        self.top_3_dt = None\n",
    "        self.top_model_lr = None\n",
    "        self.top_model_dt = None\n",
    "\n",
    "    # Defining the cross-validation method\n",
    "    def cross_validate(self, model, kfolds=10):\n",
    "        self.cv_results = cross_val_score(model, self.X_train, self.y_train, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        return self.cv_mean\n",
    "    \n",
    "    # Checking for multicollinearity\n",
    "    def check_multicollinearity(self):\n",
    "        X_train = self.X_train  # Get your training data\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "        vif[\"features\"] = X_train.columns\n",
    "        vif_sorted = vif.sort_values('VIF Factor', ascending=False)\n",
    "        return vif_sorted\n",
    "    \n",
    "    #Down Sampler\n",
    "    @staticmethod\n",
    "    def downsample(X_train, y_train, target_column, random_state=42):\n",
    "        \n",
    "        # Separate majority and minority classes\n",
    "        majority_class = X_train[X_train[target_column] == 0]\n",
    "        minority_class = X_train[X_train[target_column] == 1]\n",
    "\n",
    "        # Downsample majority class\n",
    "        majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=random_state)\n",
    "\n",
    "        # Combine minority class with downsampled majority class\n",
    "        downsampled_data = pd.concat([majority_downsampled, minority_class])\n",
    "    \n",
    "        # Separate features and target in the downsampled data\n",
    "        X_downsampled = downsampled_data.drop(target_column, axis=1)\n",
    "        y_downsampled = downsampled_data[target_column]\n",
    "\n",
    "        return X_downsampled, y_downsampled\n",
    "\n",
    "    \n",
    "    # Defining precision improving method\n",
    "    def check_precision(self, cv_now=False):\n",
    "        # Create a dictionary with ranges on the hyperparameters relevant to precision tuning\n",
    "        precision_hyperp = {\n",
    "            'C': [0.001, 0.01, 0.1, 1.0, 10],  # Adjusted range with smaller values\n",
    "            'penalty': ['l2'],  # Use 'l2' penalty for smoother decision boundaries\n",
    "            'solver': ['lbfgs', 'liblinear', 'newton-cg'],  # Common solvers for 'l2' penalty\n",
    "            'tol': [1e-5, 1e-4, 1e-3],\n",
    "            'random_state': [42],  # Set random state to 42 for reproducibility\n",
    "            'max_iter': [1000, 10000]\n",
    "        }\n",
    "\n",
    "        top_3_precision = []\n",
    "\n",
    "        # Instantiate the model (e.g., LogisticRegression or DecisionTreeClassifier) with a random state\n",
    "        model_inst = LogisticRegression(random_state=42)  \n",
    "\n",
    "        # Loop through hyperparameter combinations\n",
    "        for params in ParameterGrid(precision_hyperp):\n",
    "            try:\n",
    "                model_inst.set_params(**params)\n",
    "                model_inst.fit(self.X_train, self.y_train)\n",
    "                y_pred = model_inst.predict(self.X_test)\n",
    "                precision = precision_score(self.y_test, y_pred)  # Calculate precision score\n",
    "\n",
    "                if cv_now:\n",
    "                    cv_precision = self.cross_validate(model_inst)  # Optionally, calculate cross-validation precision\n",
    "                    top_3_precision.append((params, cv_precision, precision))\n",
    "                else:\n",
    "                    top_3_precision.append((params, precision))\n",
    "\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "        top_3_precision.sort(key=lambda x: x[-1], reverse=True)\n",
    "        top_hp_precision = top_3_precision[:1]\n",
    "\n",
    "        # Save the best hyperparameters and the tuned model\n",
    "        self.top_3_precision = top_3_precision\n",
    "        self.top_hp_precision = top_hp_precision\n",
    "        self.top_model_precision = LogisticRegression(random_state=42).set_params(**top_hp_precision[0][0]).fit(self.X_train, self.y_train)\n",
    "\n",
    "        \n",
    "          #Defining my logistic regression improving method.\n",
    "    \n",
    "    def check_log_reg(self, cv_now=False):\n",
    "        #Create a dictionary with ranges on the hyperparameters\n",
    "        log_reg_hyperp = {\n",
    "            'C': [0.01,0.1, 1.0,10,100,1000],\n",
    "            'penalty': ['l1', 'l2','elasticnet'],\n",
    "            'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky'],\n",
    "            'tol': [1e-5,1e-4, 1e-3],\n",
    "            'random_state': [420],\n",
    "            'max_iter':[1000,10000]\n",
    "    }\n",
    "        \n",
    "        top_3_lr = []\n",
    "\n",
    "        #Instantiating the model and looping through the different combinations of hyperparameters for a logistic regression model.\n",
    "        \n",
    "        model_inst = LogisticRegression()\n",
    "        for params in ParameterGrid(log_reg_hyperp):\n",
    "            try:\n",
    "                model_inst.set_params(**params)\n",
    "                model_inst.fit(self.X_train, self.y_train)\n",
    "                y_pred = model_inst.predict(self.X_test)\n",
    "                test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                if cv_now:\n",
    "                    cv_accuracy = self.cross_validate(model_inst)\n",
    "                    top_3_lr.append((params, cv_accuracy, test_accuracy))\n",
    "                else:\n",
    "                    top_3_lr.append((params, test_accuracy))\n",
    "                \n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "        top_3_lr.sort(key=lambda x: x[-1], reverse=True)\n",
    "        top_hp_lr = top_3_lr[:1]\n",
    "        top_3_lr = top_3_lr[:3]\n",
    "        \n",
    "        # Saving the attributes\n",
    "        \n",
    "        self.top_3_lr = top_3_lr\n",
    "        self.top_hp_lr = top_hp_lr\n",
    "        self.top_model_lr = LogisticRegression().set_params(**self.top_hp_lr[0][0]).fit(self.X_train,self.y_train)\n",
    "\n",
    "        \n",
    "    # Defining a model method to loop and find the best combination of hyperparameters for my decision tree\n",
    "    def check_desc_tree(self, cv_now=False):\n",
    "        # Creating a dictionary with the hyperparameters combinations\n",
    "        decision_tree_hyperp = {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'max_depth': range(1, 8),\n",
    "            'min_samples_split': range(2, 4),\n",
    "            'min_samples_leaf': range(1, 4),\n",
    "            'random_state': [42],  # Set random state to 42\n",
    "        }\n",
    "\n",
    "        top_3_dt = []\n",
    "\n",
    "        # Instantiating the model, looping through them and finding top performers\n",
    "        model_inst = DecisionTreeClassifier(random_state=42)  # Set random state to 42\n",
    "        for params in ParameterGrid(decision_tree_hyperp):\n",
    "            try:\n",
    "                model_inst.set_params(**params)\n",
    "                model_inst.fit(self.X_train, self.y_train)\n",
    "                y_pred = model_inst.predict(self.X_test)\n",
    "                test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                if cv_now:\n",
    "                    cv_accuracy = self.cross_validate(model_inst)\n",
    "                    top_3_dt.append((params, cv_accuracy, test_accuracy))\n",
    "                else:\n",
    "                    top_3_dt.append((params, test_accuracy))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "        top_3_dt.sort(key=lambda x: x[-1], reverse=True)\n",
    "        top_hp_dt = top_3_dt[:1]\n",
    "        top_3_dt = top_3_dt[:3]\n",
    "\n",
    "        # Saving attributes\n",
    "        self.top_3_dt = top_3_dt\n",
    "        self.top_hp_dt = top_hp_dt\n",
    "        self.top_model_dt = DecisionTreeClassifier(random_state=42).set_params(**self.top_hp_dt[0][0]).fit(self.X_train,\n",
    "                                                                                                           self.y_train)\n",
    "\n",
    "    # Plotting Confusion Matrix for top-performing models\n",
    "    def plot_confusion_matrix(self):\n",
    "        lr_params = self.top_hp_lr[0][0]\n",
    "        dt_params = self.top_hp_dt[0][0]\n",
    "\n",
    "        # Train the best Logistic Regression model\n",
    "        lr_model = LogisticRegression(random_state=42).set_params(**lr_params).fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Train the best Decision Tree model\n",
    "        dt_model = DecisionTreeClassifier(random_state=42).set_params(**dt_params).fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Generate confusion matrices\n",
    "        lr_cm = confusion_matrix(self.y_test, lr_model.predict(self.X_test))\n",
    "        dt_cm = confusion_matrix(self.y_test, dt_model.predict(self.X_test))\n",
    "\n",
    "        # Display confusion matrix for Logistic Regression\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        disp_lr = ConfusionMatrixDisplay(confusion_matrix=lr_cm, display_labels=['Class 0', 'Class 1'])\n",
    "        disp_lr.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "        plt.title(\"Confusion Matrix for Logistic Regression\")\n",
    "\n",
    "        # Display confusion matrix for Decision Tree\n",
    "        plt.subplot(1, 2, 2)\n",
    "        disp_dt = ConfusionMatrixDisplay(confusion_matrix=dt_cm, display_labels=['Class 0', 'Class 1'])\n",
    "        disp_dt.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "        plt.title(\"Confusion Matrix for Decision Tree\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plotting ROC Curve for top-performing models\n",
    "    def plot_roc_curve(self):\n",
    "        top_model_lr = LogisticRegression(random_state=42).set_params(**self.top_hp_lr[0][0]).fit(self.X_train, self.y_train)\n",
    "        top_model_dt = DecisionTreeClassifier(random_state=42).set_params(**self.top_hp_dt[0][0]).fit(self.X_train, self.y_train)\n",
    "\n",
    "        fpr_lr, tpr_lr, _ = roc_curve(self.y_test, top_model_lr.predict_proba(self.X_test)[:, 1])\n",
    "        roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "        fpr_dt, tpr_dt, _ = roc_curve(self.y_test, top_model_dt.predict_proba(self.X_test)[:, 1])\n",
    "        roc_auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label='Logistic Regression ROC curve (area = {:.2f})'.format(roc_auc_lr))\n",
    "        plt.plot(fpr_dt, tpr_dt, color='green', lw=2, label='Decision Tree ROC curve (area = {:.2f})'.format(roc_auc_dt))\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Determining most important features for my data\n",
    "    def feature_importance(self, n=10):\n",
    "        if self.top_hp_lr[0][-1] < self.top_hp_dt[0][-1]:\n",
    "            top_model = DecisionTreeClassifier(random_state=42).set_params(**self.top_hp_dt[0][0])\n",
    "        else:\n",
    "            top_model = LogisticRegression(random_state=42).set_params(**self.top_hp_lr[0][0])\n",
    "\n",
    "        if isinstance(self.X_train, np.ndarray):\n",
    "            # Convert X_train to a DataFrame if it's a NumPy array\n",
    "            features_used = pd.DataFrame(self.X_train, columns=self.X_train.columns)\n",
    "        else:\n",
    "            features_used = self.X_train\n",
    "\n",
    "        top_model.fit(features_used, self.y_train)\n",
    "    \n",
    "        if isinstance(self.X_train, np.ndarray):\n",
    "            feature_importance = list(zip(top_model.feature_importances_, features_used.columns))\n",
    "        else:\n",
    "            feature_importance = list(zip(abs(top_model.coef_[0]), features_used.columns))\n",
    "    \n",
    "        feature_importance.sort(key=lambda x: x[0], reverse=True)\n",
    "        return feature_importance[:n]\n",
    "\n",
    "\n",
    "    \n",
    "    # Creating a method to obtain the score DataFrame\n",
    "    def scores(self, both_models=False, include_precision=True):\n",
    "        dt_y_hat_train = self.top_model_dt.predict(self.X_train)\n",
    "        dt_y_hat_test = self.top_model_dt.predict(self.X_test)\n",
    "        lr_y_hat_train = self.top_model_lr.predict(self.X_train)\n",
    "        lr_y_hat_test = self.top_model_lr.predict(self.X_test)\n",
    "        self.cv_top_lr = self.top_hp_lr[0][1]\n",
    "        self.cv_top_dt = self.top_hp_dt[0][1]\n",
    "\n",
    "        dt_metrics = {\n",
    "            'Accuracy train': round(accuracy_score(self.y_train, dt_y_hat_train), 3),\n",
    "            'Accuracy test': round(accuracy_score(self.y_test, dt_y_hat_test), 3),\n",
    "            'Recall train': round(recall_score(self.y_train, dt_y_hat_train), 3),\n",
    "            'Recall test': round(recall_score(self.y_test, dt_y_hat_test), 3),\n",
    "            'F1 train': round(f1_score(self.y_train, dt_y_hat_train), 3),\n",
    "            'F1 test': round(f1_score(self.y_test, dt_y_hat_test), 3),\n",
    "            'CV results': round(self.cv_top_dt, 3)\n",
    "        }\n",
    "\n",
    "        lr_metrics = {\n",
    "            'Accuracy train': round(accuracy_score(self.y_train, lr_y_hat_train), 3),\n",
    "            'Accuracy test': round(accuracy_score(self.y_test, lr_y_hat_test), 3),\n",
    "            'Recall train': round(recall_score(self.y_train, lr_y_hat_train), 3),\n",
    "            'Recall test': round(recall_score(self.y_test, lr_y_hat_test), 3),\n",
    "            'F1 train': round(f1_score(self.y_train, lr_y_hat_train), 3),\n",
    "            'F1 test': round(f1_score(self.y_test, lr_y_hat_test), 3),\n",
    "            'CV results': round(self.cv_top_lr, 3)\n",
    "        }\n",
    "\n",
    "        if include_precision:\n",
    "            precision_metrics = {\n",
    "                'Precision train': round(precision_score(self.y_train, self.top_model_precision.predict(self.X_train)), 3),\n",
    "                'Precision test': round(precision_score(self.y_test, self.top_model_precision.predict(self.X_test)), 3)\n",
    "            }\n",
    "            dt_metrics.update(precision_metrics)\n",
    "            lr_metrics.update(precision_metrics)\n",
    "\n",
    "        decision_tree_df = pd.DataFrame(list(dt_metrics.values()), index=dt_metrics.keys(), columns=['Decision Tree'])\n",
    "        logistic_regression_df = pd.DataFrame(list(lr_metrics.values()), index=lr_metrics.keys(),\n",
    "                                               columns=['Logistic Regression'])\n",
    "\n",
    "        if both_models:\n",
    "            df = pd.concat([decision_tree_df, logistic_regression_df], axis=1)\n",
    "            return df\n",
    "        else:\n",
    "            if self.top_hp_lr[0][-1] < self.top_hp_dt[0][-1]:\n",
    "                return decision_tree_df\n",
    "            else:\n",
    "                return logistic_regression_df\n",
    "\n",
    "\n",
    "    # Creating a method to plot the decision tree\n",
    "    def plot_tree(self):\n",
    "        f, ax = plt.subplots(figsize=(8, 8))\n",
    "        plot_tree(self.top_model_dt, ax=ax)\n",
    "        plt.title('Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fded98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
